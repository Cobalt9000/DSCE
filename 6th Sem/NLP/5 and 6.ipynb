{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e624c377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Suprith\n",
      "[nltk_data]     Shettigar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Suprith\n",
      "[nltk_data]     Shettigar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Suprith\n",
      "[nltk_data]     Shettigar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to C:\\Users\\Suprith\n",
      "[nltk_data]     Shettigar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: John ate a delicious apple pie.\n",
      "Noun Phrases: a delicious apple, pie\n",
      "Verb Phrases: ate\n",
      "\n",
      "Sentence: The pie was baked by his mother.\n",
      "Noun Phrases: The pie, mother\n",
      "Verb Phrases: was, baked\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Sample text\n",
    "text = \"John ate a delicious apple pie. The pie was baked by his mother.\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Function to extract noun phrases\n",
    "def extract_noun_phrases(text):\n",
    "    # Tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Perform part-of-speech tagging\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Define a grammar for noun phrase chunking\n",
    "    grammar = r\"\"\"\n",
    "        NP: {<DT>?<JJ>*<NN>}   # Chunk sequences of (optional determiner), zero or more adjectives, and a noun\n",
    "    \"\"\"\n",
    "    chunk_parser = nltk.RegexpParser(grammar)\n",
    "    tree = chunk_parser.parse(tagged)\n",
    "\n",
    "    # Extract noun phrases from the tree\n",
    "    noun_phrases = [\" \".join([word for word, tag in leaf]) for leaf in tree.subtrees(filter=lambda x: x.label() == \"NP\")]\n",
    "\n",
    "    return noun_phrases\n",
    "\n",
    "# Function to extract verb phrases\n",
    "def extract_verb_phrases(text):\n",
    "    # Tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Perform part-of-speech tagging\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Define a grammar for verb phrase chunking\n",
    "    grammar = r\"\"\"\n",
    "        VP: {<VB.*><NP>?<PP>?}   # Chunk sequences of a verb, optional noun phrase, and optional prepositional phrase\n",
    "        PP: {<IN><NP>}           # Chunk sequences of a preposition and a noun phrase\n",
    "    \"\"\"\n",
    "    chunk_parser = nltk.RegexpParser(grammar)\n",
    "    tree = chunk_parser.parse(tagged)\n",
    "\n",
    "    # Extract verb phrases from the tree\n",
    "    verb_phrases = [\" \".join([word for word, tag in leaf]) for leaf in tree.subtrees(filter=lambda x: x.label() == \"VP\")]\n",
    "\n",
    "    return verb_phrases\n",
    "\n",
    "# Extract noun phrases and verb phrases from each sentence\n",
    "for sentence in sentences:\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    noun_phrases = extract_noun_phrases(sentence)\n",
    "    verb_phrases = extract_verb_phrases(sentence)\n",
    "    print(f\"Noun Phrases: {', '.join(noun_phrases)}\")\n",
    "    print(f\"Verb Phrases: {', '.join(verb_phrases)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7818a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The [delicious] [pizza] arrived, and I couldn't wait to dig in.\n",
      "\n",
      "As I [walked] along the [beach], I [heard] the [waves] crashing against the shore.\n",
      "\n",
      "In the [quiet] [library], I found myself [lost] in the pages of a [captivating] [book].\n",
      "\n",
      "The [tiny] [kitten] [meowed] [playfully], chasing after a [toy] [mouse].\n",
      "\n",
      "During the [thunderstorm], [flashes] of [lightning] [illuminated] the [dark] [sky].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define a list of prompts\n",
    "prompts = [\n",
    "    \"The [delicious] [pizza] arrived, and I couldn't wait to dig in.\",\n",
    "    \"As I [walked] along the [beach], I [heard] the [waves] crashing against the shore.\",\n",
    "    \"In the [quiet] [library], I found myself [lost] in the pages of a [captivating] [book].\",\n",
    "    \"The [tiny] [kitten] [meowed] [playfully], chasing after a [toy] [mouse].\",\n",
    "    \"During the [thunderstorm], [flashes] of [lightning] [illuminated] the [dark] [sky].\"\n",
    "]\n",
    "\n",
    "# Define a list of word categories and corresponding options\n",
    "word_categories = {\n",
    "    \"adjective\": [\"delicious\", \"quiet\", \"tiny\", \"captivating\", \"dark\"],\n",
    "    \"noun\": [\"pizza\", \"beach\", \"library\", \"kitten\", \"sky\", \"toy\", \"waves\", \"book\", \"mouse\"],\n",
    "    \"verb\": [\"arrived\", \"walked\", \"heard\", \"meowed\", \"illuminated\"],\n",
    "    \"adverb\": [\"playfully\"]\n",
    "}\n",
    "\n",
    "# Function to perform sentence completion\n",
    "def complete_sentence(prompt):\n",
    "    words_to_replace = [word for word in prompt.split() if word.startswith(\"[\") and word.endswith(\"]\")]\n",
    "    completed_sentence = prompt\n",
    "\n",
    "    for word in words_to_replace:\n",
    "        category = word[1:-1].lower()\n",
    "        if category in word_categories:\n",
    "            replacement = random.choice(word_categories[category])\n",
    "            completed_sentence = completed_sentence.replace(word, replacement, 1)\n",
    "        else:\n",
    "            # Handle case where category is not found\n",
    "            completed_sentence = completed_sentence.replace(word, f\"[{category}]\", 1)\n",
    "\n",
    "    return completed_sentence\n",
    "\n",
    "# Generate and print completed sentences\n",
    "for prompt in prompts:\n",
    "    completed_sentence = complete_sentence(prompt)\n",
    "    print(completed_sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f50a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
