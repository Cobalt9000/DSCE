{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a84bba",
   "metadata": {},
   "source": [
    "## 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "884057a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         File:        Content:\n",
      "0  sample.docx    Hi I’m Amit.\n",
      "1   sample.pdf  Hi I’m Amit.  \n",
      "2   sample.txt    Hi I'm Amit.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import docx\n",
    "import PyPDF2\n",
    "\n",
    "# Define the directory path\n",
    "dir_path = r\"C:\\Users\\amita\\Desktop\\NLP Hackathon\\files\"\n",
    "\n",
    "# List all the files in the directory with specified extensions\n",
    "files = [f for f in os.listdir(dir_path) if f.endswith('.txt') or f.endswith('.docx') or f.endswith('.pdf')]\n",
    "data = []\n",
    "\n",
    "for txt in files:\n",
    "    if txt.endswith('.txt'):\n",
    "        with open(os.path.join(dir_path,txt),'r') as file:\n",
    "            content = file.read()\n",
    "            data.append({'File:':txt,'Content:':content})\n",
    "    elif txt.endswith('.docx'):\n",
    "        docx_file = os.path.join(dir_path,txt)\n",
    "        doc = docx.Document(docx_file)\n",
    "        content = \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "        data.append({'File:':txt,'Content:':content})\n",
    "    elif txt.endswith('.pdf'):\n",
    "        with open(os.path.join(dir_path,txt),'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            content = \"\"\n",
    "            for page in range(num_pages):\n",
    "                content+=pdf_reader.pages[page].extract_text()\n",
    "            data.append({'File:':txt,'Content:':content})\n",
    "        \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba3e904",
   "metadata": {},
   "source": [
    "## 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a6d6d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        content    cleaned_content\n",
      "0  This is an example sentence.  example sentence.\n",
      "1     Another example sentence.  example sentence.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample data creation (replace this with your actual data loading logic)\n",
    "data = {'content': [\"This is an example sentence.\", \"Another example sentence.\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def clean(text):\n",
    "    re.sub(r'[^A-Za-z\\s]','',text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    stop = set(['this','is','an','another'])\n",
    "    tokens = [word for word in tokens if word not in stop]\n",
    "    tokens = [word[:-1] if word.endswith('s') else word for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['cleaned_content'] = df['content'].apply(clean)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5051a",
   "metadata": {},
   "source": [
    "## 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3787c5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               cleaned_content                                     bigrams\n",
      "0  this is an example sentence  [thisis, isan, anexample, examplesentence]\n",
      "1     another example sentence           [anotherexample, examplesentence]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data creation\n",
    "data = {'cleaned_content': [\"this is an example sentence\", \"another example sentence\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to generate bigrams\n",
    "def bii(text,n):\n",
    "    tokens = text.split()\n",
    "    ngram_list = [\"\".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    return ngram_list\n",
    "# Apply the function to generate bigrams\n",
    "df['bigrams'] = df['cleaned_content'].apply(bii, n=2)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1e28991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          bigrams\n",
      "0  [this is, is an, an example, example sentence]\n",
      "1             [another example, example sentence]\n",
      "                                           trigrams\n",
      "0  [this is an, is an example, an example sentence]\n",
      "1                        [another example sentence]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data creation\n",
    "data = {'cleaned_content': [\"this is an example sentence\", \"another example sentence\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to generate ngrams\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    ngrams_list = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    return ngrams_list\n",
    "\n",
    "# Apply the function to generate bigrams and trigrams\n",
    "df1 = pd.DataFrame()\n",
    "df1['bigrams'] = df['cleaned_content'].apply(generate_ngrams, n=2)\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "df2['trigrams'] = df['cleaned_content'].apply(generate_ngrams, n=3)\n",
    "\n",
    "# Print the dataframes\n",
    "print(df1)\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72080ecd",
   "metadata": {},
   "source": [
    "## 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebe4e9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               cleaned_content  \\\n",
      "0  this is an example sentence   \n",
      "1     another example sentence   \n",
      "\n",
      "                                            pos_tags  \n",
      "0  [(this, Noun), (is, Noun), (an, Det), (example...  \n",
      "1  [(another, Noun), (example, Noun), (sentence, ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data creation\n",
    "data = {'cleaned_content': [\"this is an example sentence\", \"another example sentence\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to generate simple POS tags (Noun, Verb, Adjective)\n",
    "\n",
    "def psss(text):\n",
    "    tokens = text.split()\n",
    "    poss = []\n",
    "    for token in tokens:\n",
    "        if token.endswith('ing'):\n",
    "            poss.append((token,'Verb'))\n",
    "        elif token in ['a','an']:\n",
    "            poss.append((token,'Det'))\n",
    "        else:\n",
    "            poss.append((token,'Noun'))\n",
    "    return poss\n",
    "\n",
    "df['pos_tags'] = df['cleaned_content'].apply(psss)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba8e8e",
   "metadata": {},
   "source": [
    "## 5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee6dc111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            POS_tags  \\\n",
      "0  [(this, Noun), (is, Noun), (an, Noun), (exampl...   \n",
      "1  [(another, Noun), (example, Noun), (sentence, ...   \n",
      "\n",
      "                    noun_phrases  \n",
      "0  [this is an example sentence]  \n",
      "1     [another example sentence]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data creation\n",
    "data = {'POS_tags': [\n",
    "[('this', 'Noun'), ('is', 'Noun'), ('an', 'Noun'), ('example', 'Noun'), ('sentence', 'Noun')],\n",
    "[('another', 'Noun'), ('example', 'Noun'), ('sentence', 'Noun')]\n",
    "]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to perform simple noun phrase chunking\n",
    "def noumn(pos_tags):\n",
    "    np = []\n",
    "    cp = []\n",
    "    for token,tag in pos_tags:\n",
    "        if tag in ['Noun']:\n",
    "            cp.append(token)\n",
    "        elif cp:\n",
    "            np.append(' '.join(cp))\n",
    "            cp = []\n",
    "    if cp:\n",
    "        np.append(' '.join(cp))\n",
    "    return np\n",
    "# Apply the function to generate noun phrases\n",
    "df['noun_phrases'] = df['POS_tags'].apply(noumn)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6640cc4",
   "metadata": {},
   "source": [
    "## 6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "352e75e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible\n",
      "-After a long day at work, I like to relax by watching my favorite TV show\n",
      "-After a long day at work, I like to relax by going for a walk\n",
      "-After a long day at work, I like to relax by reading a book\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sentence_prompts = {\n",
    "    \"She opened the door and saw a\": [\"beautiful garden\", \"mysterious figure\", \"bright light\"],\n",
    "    \"After a long day at work, I like to relax by\": [\"watching my favorite TV show\", \"going for a walk\", \"reading a book\"]\n",
    "}\n",
    "input_prompt = \"After a long day at work, I like to relax by\"\n",
    "if input_prompt in sentence_prompts:\n",
    "    possible = sentence_prompts[input_prompt]\n",
    "    print(\"Possible\")\n",
    "    for com in possible:\n",
    "        print(f\"-{input_prompt} {com}\")\n",
    "else:\n",
    "    rand = random.choice([\"Hello\"])\n",
    "    print(f\"-{input_prompt} {rand}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cd226cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt not found in the dictionary.\n",
      "- After a long day at work, I like to relax by by enjoying a cup of tea\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sentence_prompts = {\n",
    "    \"She opened the door and saw a\": [\"beautiful garden\", \"mysterious figure\", \"bright light\"],\n",
    "    \"After a long day at work, I like to relax by\": [\"watching my favorite TV show\", \"going for a walk\", \"reading a book\"]\n",
    "}\n",
    "input_prompt = \"After a long day at work, I like to relax by by\"\n",
    "if input_prompt in sentence_prompts:\n",
    "    possible_completions = sentence_prompts[input_prompt]\n",
    "    print(\"Possible Completions:\")\n",
    "    for completion in possible_completions:\n",
    "        print(f\"- {input_prompt} {completion}\")\n",
    "else:\n",
    "    print(\"Prompt not found in the dictionary.\")\n",
    "    # Use random to create a random sentence completion\n",
    "    random_completion = random.choice([\"enjoying a cup of tea\", \"listening to music\", \"playing video games\"])\n",
    "    print(f\"- {input_prompt} {random_completion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d2112",
   "metadata": {},
   "source": [
    "## 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9397aff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this product!\n",
      "Label: positive\n",
      "Text: It's terrible.\n",
      "Label: negative\n",
      "Text: Neutral statement.\n",
      "Label: neutral\n"
     ]
    }
   ],
   "source": [
    "data = [\"I love this product!\", \"It's terrible.\", \"Neutral statement.\"]\n",
    "\n",
    "def sen(text):\n",
    "    text = text.lower()\n",
    "    if 'love' in text:\n",
    "        return 'positive'\n",
    "    elif 'terrible' in text:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "result_dict = ({'text':data,'label':[sen(text) for text in data]})\n",
    "\n",
    "for text,data in zip(result_dict['text'],result_dict['label']):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Label: {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abeecdd",
   "metadata": {},
   "source": [
    "## 8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "238af1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON - The Trump administration has ordered the military to start withdrawing roughly 7,000 troops from Afghanistan in\n",
      "the coming months, two defense officials said Thursday, an abrupt shift in the 17-year-old war there and a decision that stunned Afghan officials, who\n",
      "said they had not been briefed on the plans.Though Pentagon officials have said the influx of forces - coupled with a more aggressive air campaign - was helping the war effort, Afghan forces\n",
      "continued to take nearly unsustainable levels of casualties and lose ground to the Taliban.The announcement came hours after Jim Mattis, the secretary of defense, said that he would resign from his position at the end of February after\n",
      "disagreeing with the president over his approach to policy in the Middle East.\n"
     ]
    }
   ],
   "source": [
    "def summarizerr(article,num_sentences=3):\n",
    "    sentences = article.split('.')\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    scores = [len(sentence) for sentence in sentences]\n",
    "    selected = sorted(zip(sentences,scores),key=lambda x:x[1],reverse=True)[:num_sentences]\n",
    "    summary = [sentence for sentence,_ in selected]\n",
    "    return '.'.join(summary)+'.'\n",
    "\n",
    "article = \"\"\"\n",
    "WASHINGTON - The Trump administration has ordered the military to start withdrawing roughly 7,000 troops from Afghanistan in\n",
    "the coming months, two defense officials said Thursday, an abrupt shift in the 17-year-old war there and a decision that stunned Afghan officials, who\n",
    "said they had not been briefed on the plans.\n",
    "President Trump made the decision to pull the troops - about half the number the United States has in Afghanistan now - at the same time he\n",
    "decided to pull American forces out of Syria, one official said.\n",
    "The announcement came hours after Jim Mattis, the secretary of defense, said that he would resign from his position at the end of February after\n",
    "disagreeing with the president over his approach to policy in the Middle East.\n",
    "The whirlwind of troop withdrawals and the resignation of Mr. Mattis leave a murky picture for what is next in the United States’ longest war, and they\n",
    "come as Afghanistan has been troubled by spasms of violence afflicting the capital, Kabul, and other important areas.\n",
    "The United States has also been conducting talks with representatives of the Taliban, in what officials have described as discussions that could\n",
    "lead to formal talks to end the conflict.\n",
    "Senior Afghan officials and Western diplomats in Kabul woke up to the shock of the news on Friday morning, and many of them braced for chaos ahead.\n",
    "Several Afghan officials, often in the loop on security planning and decision-making, said they had received no indication in recent days that\n",
    "the Americans would pull troops out.\n",
    "The fear that Mr. Trump might take impulsive actions, however, often loomed in the background of discussions with the United States, they said.\n",
    "They saw the abrupt decision as a further sign that voices from the ground were lacking in the debate over the war and that with Mr. Mattis’s\n",
    "resignation, Afghanistan had lost one of the last influential voices in Washington who channeled the reality of the conflict into the White House’s deliberations.\n",
    "The president long campaigned on bringing troops home, but in 2017, at the request of Mr. Mattis, he begrudgingly pledged an additional 4,000\n",
    "troops to the Afghan campaign to try to hasten an end to the conflict.\n",
    "Though Pentagon officials have said the influx of forces - coupled with a more aggressive air campaign - was helping the war effort, Afghan forces\n",
    "continued to take nearly unsustainable levels of casualties and lose ground to the Taliban.\n",
    "The renewed American effort in 2017 was the first step in ensuring Afghan forces could become more independent without a set timeline for\n",
    "a withdrawal.\n",
    "But with plans to quickly reduce the number of American troops in the country, it is unclear if the Afghans can hold their own against an\n",
    "increasingly aggressive Taliban.\n",
    "Currently, American airstrikes are at levels not seen since the height of the war, when tens of thousands of American troops were spread\n",
    "throughout the country.\n",
    "That air support, officials say, consists mostly of propping up Afghan troops while they try to hold territory from a resurgent Taliban.\n",
    "\"\"\"\n",
    "\n",
    "# Perform summarization\n",
    "summary =  summarizerr(article)\n",
    "\n",
    "# Print the summary\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eea3af",
   "metadata": {},
   "source": [
    "## 9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "caa69ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'Obama']\n",
      "['Hawaii', 'United', 'States']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Barack Obama was born in Hawaii and served as the 44th President of the United States\"\n",
    "\n",
    "# Initialize lists\n",
    "person = []\n",
    "place = []\n",
    "\n",
    "entities = sentence.split()\n",
    "for entity in entities:\n",
    "    if entity in ['Barack','Obama']:\n",
    "        person.append(entity)\n",
    "    elif entity in ['Hawaii','United','States']:\n",
    "        place.append(entity)\n",
    "print(person)\n",
    "print(place)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9088b6",
   "metadata": {},
   "source": [
    "## 10b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ead6c851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs']\n",
      "Stemmed words: ['The', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'the', 'lazy', 'dog']\n",
      "Lemmatized words: ['The', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'the', 'lazy', 'dog']\n",
      "\n",
      "\n",
      "Word: misunderstanding\n",
      "Morphemes: ['mis', 'understand', 'ing']\n"
     ]
    }
   ],
   "source": [
    "def simple_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def simple_porter_stemmer(word):\n",
    "    # A simple stemming function (for illustration purposes)\n",
    "    if word.endswith(\"es\"):\n",
    "        return word[:-2]\n",
    "    elif word.endswith(\"s\"):\n",
    "        return word[:-1]\n",
    "    elif word.endswith(\"ing\"):\n",
    "        return word[:-3]\n",
    "    return word\n",
    "\n",
    "def simple_wordnet_lemmatizer(word):\n",
    "    # A simple lemmatization function (for illustration purposes)\n",
    "    if word.endswith(\"es\"):\n",
    "        return word[:-2]\n",
    "    elif word.endswith(\"s\"):\n",
    "        return word[:-1]\n",
    "    elif word.endswith(\"ing\"):\n",
    "        return word[:-3]\n",
    "    return word\n",
    "\n",
    "def analyze_morphemes(word, prefixes, root, suffixes):\n",
    "    morphemes = []\n",
    "    for prefix in prefixes:\n",
    "        if word.startswith(prefix):\n",
    "            morphemes.append(prefix)\n",
    "            word = word[len(prefix):]\n",
    "    morphemes.append(root)\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            morphemes.append(suffix)\n",
    "            word = word[:-len(suffix)]\n",
    "    return morphemes\n",
    "\n",
    "text = \"The quick brown foxes are jumping over the lazy dogs\"\n",
    "words = simple_tokenizer(text)\n",
    "stemmed_words = [simple_porter_stemmer(word) for word in words]\n",
    "lemmatized_words = [simple_wordnet_lemmatizer(word) for word in words]\n",
    "\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)\n",
    "print(\"Lemmatized words:\", lemmatized_words)\n",
    "print(\"\\n\")\n",
    "\n",
    "word = \"misunderstanding\"\n",
    "prefixes = [\"mis\"]\n",
    "root = \"understand\"\n",
    "suffixes = [\"ing\"]\n",
    "morphemes = analyze_morphemes(word, prefixes, root, suffixes)\n",
    "\n",
    "print(\"Word:\", word)\n",
    "print(\"Morphemes:\", morphemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a6c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
