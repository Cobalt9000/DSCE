{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4176a1b3",
   "metadata": {},
   "source": [
    "# Program 1:\n",
    "\n",
    "Study of Python and basic commands to access text data. \n",
    "(from notepad, pdf, word documents, online)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8db45b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (2.0.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: python-docx in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas python-docx PyPDF2 requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d06c8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text file for first program.\n"
     ]
    }
   ],
   "source": [
    "# Reading from a text file\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Writing to a text file\n",
    "def write_text_file(file_path, content):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Example usage\n",
    "file_path = 'test.txt'\n",
    "text_content = read_text_file(file_path)\n",
    "print(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e612b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text file for first program.\n"
     ]
    }
   ],
   "source": [
    "new_content = \"This is a text file for first program.\"\n",
    "write_text_file('test.txt', new_content)\n",
    "text_content = read_text_file(file_path)\n",
    "print(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c2e2af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text file for first program.\n"
     ]
    }
   ],
   "source": [
    "#for a text file\n",
    "with open('test.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af9e122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e1c076d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a pdf file for lab program 1.  \n"
     ]
    }
   ],
   "source": [
    "#For PDF files\n",
    "import PyPDF2\n",
    "\n",
    "with open('pdf.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    text = reader.pages[0].extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e167254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\suprith shettigar\\anaconda3\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f704dc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is a word(docx) file for lab program 1.\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "doc = docx.Document('word.docx')\n",
    "textdoc = ' '\n",
    "for paragraph in doc.paragraphs:\n",
    "    textdoc += paragraph.text\n",
    "print(textdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c8c18b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Domain\n",
      "Example Domain\n",
      "This domain is for use in illustrative examples in documents. You may use this\n",
      "domain in literature without prior coordination or asking for permission.\n",
      "More information...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://example.com'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Remove script and style elements\n",
    "for script in soup([\"script\", \"style\"]):\n",
    "    script.decompose()\n",
    "\n",
    "# Get text\n",
    "text = soup.get_text()\n",
    "\n",
    "# Break into lines and remove leading and trailing space on each\n",
    "lines = (line.strip() for line in text.splitlines())\n",
    "# Break multi-headlines into a line each\n",
    "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "# Drop blank lines\n",
    "content = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2394de23",
   "metadata": {},
   "source": [
    "## OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d513c316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         File:                                           Content:\n",
      "0      pdf.pdf            This is a pdf file for lab program 1.  \n",
      "1     test.txt             This is a text file for first program.\n",
      "2    word.docx       This is a word(docx) file for lab program 1.\n",
      "3  example.com  Example Domain\\nExample Domain\\nThis domain is...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import docx\n",
    "import PyPDF2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Define the directory path\n",
    "dir_path = r\"C:\\Users\\Suprith Shettigar\\OneDrive\\Desktop\\college\\NLP\\Data\"\n",
    "\n",
    "# List all the files in the directory with specified extensions and URLs\n",
    "files = [f for f in os.listdir(dir_path) if f.endswith(('.txt', '.docx', '.pdf'))]\n",
    "urls = ['https://example.com']  # Add your URLs here\n",
    "data = []\n",
    "\n",
    "def get_online_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    text = soup.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    content = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return content\n",
    "\n",
    "for item in files + urls:\n",
    "    if isinstance(item, str) and item.startswith('http'):\n",
    "        content = get_online_content(item)\n",
    "        filename = urlparse(item).netloc\n",
    "        data.append({'File:': filename, 'Content:': content})\n",
    "    elif item.endswith('.txt'):\n",
    "        with open(os.path.join(dir_path, item), 'r') as file:\n",
    "            content = file.read()\n",
    "            data.append({'File:': item, 'Content:': content})\n",
    "    elif item.endswith('.docx'):\n",
    "        docx_file = os.path.join(dir_path, item)\n",
    "        doc = docx.Document(docx_file)\n",
    "        content = \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "        data.append({'File:': item, 'Content:': content})\n",
    "    elif item.endswith('.pdf'):\n",
    "        with open(os.path.join(dir_path, item), 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            content = \"\"\n",
    "            for page in range(num_pages):\n",
    "                content += pdf_reader.pages[page].extract_text()\n",
    "            data.append({'File:': item, 'Content:': content})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c13e7",
   "metadata": {},
   "source": [
    "# Program 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bcb648",
   "metadata": {},
   "source": [
    "Perform text pre - processing on a given corpus without using any pre - defined NLP packages. \n",
    "\n",
    "\n",
    "The text corpus given below: \n",
    "\n",
    "“The weather was beautiful. I went for a walk in the park. It was a sunny day, and the birds were chirping happily. \n",
    "\n",
    "\n",
    "Suddenly, a black cat crossed my path. I stopped and watched it disappear into the bushes. \n",
    "\n",
    "\n",
    "After that, I continued my stroll, enjoying the tranquility of nature.” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ebe5d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  The weather was beautiful. I went for a walk i...   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  weather beautiful. went walk park. sunny day, ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample data creation (replace this with your actual data loading logic)\n",
    "# added the content given.\n",
    "data = {'content': [\"The weather was beautiful. I went for a walk in the park. It was a sunny day, and the birds were chirping happily. Suddenly, a black cat crossed my path. I stopped and watched it disappear into the bushes. After that, I continued my stroll, enjoying the tranquility of nature.\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def clean(text):\n",
    "    re.sub(r'[^A-Za-z\\s]','',text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    stop = set(['the', 'was', 'i', 'for', 'a', 'in', 'the', 'it', 'and', 'were', 'my', 'it', 'into', 'after', 'that', 'of'])\n",
    "    tokens = [word for word in tokens if word not in stop]\n",
    "    tokens = [word[:-1] if word.endswith('s') else word for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['cleaned_content'] = df['content'].apply(clean)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d3fe08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        content    cleaned_content\n",
      "0  This is an example sentence.  example sentence.\n",
      "1     Another example sentence.  example sentence.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample data creation (replace this with your actual data loading logic)\n",
    "data = {'content': [\"This is an example sentence.\", \"Another example sentence.\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def clean(text):\n",
    "    re.sub(r'[^A-Za-z\\s]','',text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    stop = set(['this','is','an','another'])\n",
    "    tokens = [word for word in tokens if word not in stop]\n",
    "    tokens = [word[:-1] if word.endswith('s') else word for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['cleaned_content'] = df['content'].apply(clean)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ebd82d",
   "metadata": {},
   "source": [
    "# Program 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d6b06c",
   "metadata": {},
   "source": [
    "Implement N -Gram model in python without using any predefined NLP \n",
    "packages. \n",
    "\n",
    "Note: use corpus of your own choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0533965c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               cleaned_content                                     bigrams\n",
      "0  this is an example sentence  [thisis, isan, anexample, examplesentence]\n",
      "1     another example sentence           [anotherexample, examplesentence]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data creation\n",
    "data = {'cleaned_content': [\"this is an example sentence\", \"another example sentence\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to generate bigrams\n",
    "def bii(text,n):\n",
    "    tokens = text.split()\n",
    "    ngram_list = [\"\".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    return ngram_list\n",
    "# Apply the function to generate bigrams\n",
    "df['bigrams'] = df['cleaned_content'].apply(bii, n=2)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec861f5",
   "metadata": {},
   "source": [
    "## OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e7b1ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          bigrams\n",
      "0  [this is, is an, an example, example sentence]\n",
      "1             [another example, example sentence]\n",
      "                                           trigrams\n",
      "0  [this is an, is an example, an example sentence]\n",
      "1                        [another example sentence]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data creation\n",
    "data = {'cleaned_content': [\"this is an example sentence\", \"another example sentence\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to generate ngrams\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    ngrams_list = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    return ngrams_list\n",
    "\n",
    "# Apply the function to generate bigrams and trigrams\n",
    "df1 = pd.DataFrame()\n",
    "df1['bigrams'] = df['cleaned_content'].apply(generate_ngrams, n=2)\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "df2['trigrams'] = df['cleaned_content'].apply(generate_ngrams, n=3)\n",
    "\n",
    "# Print the dataframes\n",
    "print(df1)\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa69e3b",
   "metadata": {},
   "source": [
    "# Program 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b0a72e",
   "metadata": {},
   "source": [
    "Write a Python program to perform Part-of-Speech (POS) tagging on a \n",
    "given text corpus without using any predefined NLP packages. \n",
    "The text corpus and corresponding part-of-speech tags are provided below: \n",
    "\n",
    "“The cat chased the mouse around the house. Birds sang in the trees while the \n",
    "sun shone brightly in the sky. A group of children played happily in the park, \n",
    "laughing and running around.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ceb1dd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     cleaned_content  \\\n",
      "0  The cat chased the mouse around the house. Bir...   \n",
      "\n",
      "                                            pos_tags  \n",
      "0  [(The, Noun), (cat, Noun), (chased, Noun), (th...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data creation (added the content given)\n",
    "data = {'cleaned_content': [\"The cat chased the mouse around the house. Birds sang in the trees while the sun shone brightly in the sky. A group of children played happily in the park, laughing and running around.\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to generate simple POS tags (Noun, Verb, Adjective)\n",
    "\n",
    "def psss(text):\n",
    "    tokens = text.split()\n",
    "    poss = []\n",
    "    for token in tokens:\n",
    "        if token.endswith('ing'):\n",
    "            poss.append((token,'Verb'))\n",
    "        elif token in ['a','an']:\n",
    "            poss.append((token,'Det'))\n",
    "        else:\n",
    "            poss.append((token,'Noun'))\n",
    "    return poss\n",
    "\n",
    "df['pos_tags'] = df['cleaned_content'].apply(psss)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "79e2be67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               cleaned_content  \\\n",
      "0  this is an example sentence   \n",
      "1     another example sentence   \n",
      "\n",
      "                                            pos_tags  \n",
      "0  [(this, Noun), (is, Noun), (an, Det), (example...  \n",
      "1  [(another, Noun), (example, Noun), (sentence, ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data creation\n",
    "data = {'cleaned_content': [\"this is an example sentence\", \n",
    "\"another example sentence\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to generate simple POS tags (Noun, Verb, Adjective)\n",
    "\n",
    "def psss(text):\n",
    "    tokens = text.split()\n",
    "    poss = []\n",
    "    for token in tokens:\n",
    "        if token.endswith('ing'):\n",
    "            poss.append((token,'Verb'))\n",
    "        elif token in ['a','an']:\n",
    "            poss.append((token,'Det'))\n",
    "        else:\n",
    "            poss.append((token,'Noun'))\n",
    "    return poss\n",
    "\n",
    "df['pos_tags'] = df['cleaned_content'].apply(psss)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255e0ef",
   "metadata": {},
   "source": [
    "# Program 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b39b62",
   "metadata": {},
   "source": [
    "Implement chunking to extract Noun and Verb phrases without using any \n",
    "pre defined NLP packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a5c60d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            POS_tags  \\\n",
      "0  [(The, DET), (quick, ADJ), (brown, NOUN), (fox...   \n",
      "1  [(A, DET), (cat, NOUN), (is, AUX), (sleeping, ...   \n",
      "\n",
      "                                    NP                        VP  \n",
      "0  [The quick brown fox, the lazy dog]                   [jumps]  \n",
      "1                              [A cat]  [is sleeping peacefully]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'POS_tags': [\n",
    "        [('The', 'DET'), ('quick', 'ADJ'), ('brown', 'NOUN'), ('fox', 'NOUN'), \n",
    "         ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), \n",
    "         ('dog', 'NOUN')],\n",
    "        [('A', 'DET'), ('cat', 'NOUN'), ('is', 'AUX'), ('sleeping', 'VERB'), \n",
    "         ('peacefully', 'ADV')]\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def chunk_phrases(pos_tags):\n",
    "    np, vp, current = [], [], []\n",
    "    for token, tag in pos_tags:\n",
    "        if tag in ['DET', 'ADJ', 'NOUN']:\n",
    "            if current and current[0][1] in ['VERB', 'AUX', 'ADV']:\n",
    "                vp.append(' '.join(t for t, _ in current))\n",
    "                current = []\n",
    "            current.append((token, tag))\n",
    "        elif tag in ['VERB', 'AUX', 'ADV']:\n",
    "            if current and current[0][1] in ['DET', 'ADJ', 'NOUN']:\n",
    "                np.append(' '.join(t for t, _ in current))\n",
    "                current = []\n",
    "            current.append((token, tag))\n",
    "        else:\n",
    "            if current:\n",
    "                (np if current[0][1] in ['DET', 'ADJ', 'NOUN'] \n",
    "                 else vp).append(' '.join(t for t, _ in current))\n",
    "                current = []\n",
    "    if current:\n",
    "        (np if current[0][1] in ['DET', 'ADJ', 'NOUN'] \n",
    "         else vp).append(' '.join(t for t, _ in current))\n",
    "    return np, vp\n",
    "\n",
    "df['NP'], df['VP'] = zip(*df['POS_tags'].apply(chunk_phrases))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4a69dd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun Phrases: ['there was a clown']\n",
      "Verb Phrases: ['running fast is important for survival in the wild']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_phrases(text):\n",
    "    # Tokenize and tag\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    tagged = [(w, 'NN' if re.search(r'(ness|ment|ship|ity|tion|ism|er|or)$', w) else\n",
    "               'VB' if re.search(r'(ed|ing|es|ize|ise|ate|fy|en)$', w) else\n",
    "               'JJ' if re.search(r'(able|ible|al|ful|ic|ive|less|ous)$', w) else 'NN') for w in words]\n",
    "    \n",
    "    # Chunk\n",
    "    np, vp = [], []\n",
    "    current_np, current_vp = [], []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(tagged):\n",
    "        word, tag = tagged[i]\n",
    "        if tag in ['NN', 'JJ']:\n",
    "            if current_vp: \n",
    "                vp.append(' '.join(current_vp))\n",
    "                current_vp = []\n",
    "            current_np.append(word)\n",
    "        elif tag == 'VB':\n",
    "            if current_np: \n",
    "                np.append(' '.join(current_np))\n",
    "                current_np = []\n",
    "            current_vp = [word]\n",
    "            # Look ahead for potential objects of the verb\n",
    "            j = i + 1\n",
    "            while j < len(tagged) and tagged[j][1] in ['NN', 'JJ']:\n",
    "                current_vp.append(tagged[j][0])\n",
    "                j += 1\n",
    "            i = j - 1  # Move the index to the last word added to the verb phrase\n",
    "        else:\n",
    "            if current_np: \n",
    "                np.append(' '.join(current_np))\n",
    "                current_np = []\n",
    "            if current_vp: \n",
    "                vp.append(' '.join(current_vp))\n",
    "                current_vp = []\n",
    "        i += 1\n",
    "    \n",
    "    if current_np: np.append(' '.join(current_np))\n",
    "    if current_vp: vp.append(' '.join(current_vp))\n",
    "    \n",
    "    return np, vp\n",
    "\n",
    "# Example usage\n",
    "text = \"There was a clown. Running fast is important for survival in the wild.\"\n",
    "noun_phrases, verb_phrases = extract_phrases(text)\n",
    "print(\"Noun Phrases:\", noun_phrases)\n",
    "print(\"Verb Phrases:\", verb_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c8dfd1",
   "metadata": {},
   "source": [
    "# Program 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ba360",
   "metadata": {},
   "source": [
    "Sentence completion with words or phrases using random \n",
    "prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d6efdffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible Completions:\n",
      "- After a long day at work, I like to relax by watching my favorite TV show\n",
      "- After a long day at work, I like to relax by going for a walk\n",
      "- After a long day at work, I like to relax by reading a book\n",
      "\n",
      " test run:\n",
      "- After a long day at work, I like to relax by listening to music\n"
     ]
    }
   ],
   "source": [
    "sentence_prompts = {\n",
    "    \"She opened the door and saw a\": [\"beautiful garden\", \"mysterious figure\", \"bright light\"],\n",
    "    \"After a long day at work, I like to relax by\": [\"watching my favorite TV show\", \"going for a walk\", \"reading a book\"]\n",
    "}\n",
    "\n",
    "input_prompt = \"After a long day at work, I like to relax by\"\n",
    "\n",
    "if input_prompt in sentence_prompts:\n",
    "    possible_completions = sentence_prompts[input_prompt]\n",
    "    print(\"Possible Completions:\")\n",
    "    for completion in possible_completions:\n",
    "        print(f\"- {input_prompt} {completion}\")\n",
    "else:\n",
    "    print(\"Prompt not found in the dictionary.\")\n",
    "\n",
    "# Use random to create a random sentence completion\n",
    "random_completion = random.choice([\"enjoying a cup of tea\", \"listening to music\", \"playing video games\"])\n",
    "\n",
    "print(\"\\n test run:\")\n",
    "print(f\"- {input_prompt} {random_completion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e6b5dc",
   "metadata": {},
   "source": [
    "## OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ab03b7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible\n",
      "-After a long day at work, I like to relax by watching my favorite TV show\n",
      "-After a long day at work, I like to relax by going for a walk\n",
      "-After a long day at work, I like to relax by reading a book\n"
     ]
    }
   ],
   "source": [
    "sentence_prompts = {\n",
    "    \"She opened the door and saw a\": [\"beautiful garden\", \"mysterious figure\", \"bright light\"],\n",
    "    \"After a long day at work, I like to relax by\": [\"watching my favorite TV show\", \"going for a walk\", \"reading a book\"]\n",
    "}\n",
    "input_prompt = \"After a long day at work, I like to relax by\"\n",
    "if input_prompt in sentence_prompts:\n",
    "    possible = sentence_prompts[input_prompt]\n",
    "    print(\"Possible\")\n",
    "    for com in possible:\n",
    "        print(f\"-{input_prompt} {com}\")\n",
    "else:\n",
    "    rand = random.choice([\"Hello\"])\n",
    "    print(f\"-{input_prompt} {rand}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ec3162",
   "metadata": {},
   "source": [
    "# Program 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e844a",
   "metadata": {},
   "source": [
    "Implement machine learning sentiment classification without \n",
    "using any pre defined NLP packages: \n",
    "\n",
    "Training Corpus: \n",
    "\n",
    "(\"I love this product\", \"positive\"), \n",
    "(\"This is excellent\", \"positive\"), \n",
    "(\"Terrible service\", \"negative\"), \n",
    "(\"It's okay, not great\", \"neutral\"), \n",
    "(\"Amazing experience\", \"positive\"), \n",
    "(\"Disappointing outcome\", \"negative\"), \n",
    "(\"Neutral feelings\", \"neutral\"), \n",
    "(\"I dislike it\", \"negative\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d6b6c3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'I really enjoy this' - Sentiment: positive\n",
      "Sentence: 'This is awful' - Sentiment: positive\n",
      "Sentence: 'I'm not sure how I feel about this' - Sentiment: positive\n",
      "Sentence: 'Fantastic product, highly recommended' - Sentiment: negative\n",
      "Sentence: 'Very disappointing experience' - Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "class SentimentClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_counts = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocabulary = set()\n",
    "\n",
    "    def train(self, training_data):\n",
    "        for text, sentiment in training_data:\n",
    "            if sentiment not in self.class_counts:\n",
    "                self.class_counts[sentiment] = 0\n",
    "                self.word_counts[sentiment] = {}\n",
    "            \n",
    "            self.class_counts[sentiment] += 1\n",
    "            words = text.lower().split()\n",
    "            \n",
    "            for word in words:\n",
    "                if word not in self.word_counts[sentiment]:\n",
    "                    self.word_counts[sentiment][word] = 0\n",
    "                self.word_counts[sentiment][word] += 1\n",
    "                self.vocabulary.add(word)\n",
    "\n",
    "    def classify(self, text):\n",
    "        words = text.lower().split()\n",
    "        scores = {}\n",
    "        total_docs = sum(self.class_counts.values())\n",
    "\n",
    "        for sentiment in self.class_counts:\n",
    "            score = self.class_counts[sentiment] / total_docs\n",
    "            for word in words:\n",
    "                word_count = self.word_counts[sentiment].get(word, 0)\n",
    "                total_words = sum(self.word_counts[sentiment].values())\n",
    "                word_prob = (word_count + 1) / (total_words + len(self.vocabulary))\n",
    "                score *= word_prob\n",
    "            scores[sentiment] = score\n",
    "\n",
    "        return max(scores, key=scores.get)\n",
    "\n",
    "# Training data\n",
    "training_corpus = [\n",
    "    (\"I love this product\", \"positive\"),\n",
    "    (\"This is excellent\", \"positive\"),\n",
    "    (\"Terrible service\", \"negative\"),\n",
    "    (\"It's okay, not great\", \"neutral\"),\n",
    "    (\"Amazing experience\", \"positive\"),\n",
    "    (\"Disappointing outcome\", \"negative\"),\n",
    "    (\"Neutral feelings\", \"neutral\"),\n",
    "    (\"I dislike it\", \"negative\")\n",
    "]\n",
    "\n",
    "# Create and train the classifier\n",
    "classifier = SentimentClassifier()\n",
    "classifier.train(training_corpus)\n",
    "\n",
    "# Test the classifier\n",
    "test_sentences = [\n",
    "    \"I really enjoy this\",\n",
    "    \"This is awful\",\n",
    "    \"I'm not sure how I feel about this\",\n",
    "    \"Fantastic product, highly recommended\",\n",
    "    \"Very disappointing experience\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    sentiment = classifier.classify(sentence)\n",
    "    print(f\"Sentence: '{sentence}' - Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5a23d084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this product\n",
      "Label: positive\n",
      "Text: This is excellent\n",
      "Label: positive\n",
      "Text: Terrible service\n",
      "Label: negative\n",
      "Text: It's okay, not great\n",
      "Label: neutral\n",
      "Text: Amazing experience\n",
      "Label: neutral\n",
      "Text: Disappointing outcome\n",
      "Label: neutral\n",
      "Text: Neutral feelings\n",
      "Label: neutral\n",
      "Text: I dislike it\n",
      "Label: negative\n"
     ]
    }
   ],
   "source": [
    "data = [\"I love this product\", \"This is excellent\", \"Terrible service\", \n",
    "        \"It's okay, not great\", \"Amazing experience\", \"Disappointing outcome\",\n",
    "        \"Neutral feelings\", \"I dislike it\"]\n",
    "\n",
    "def sen(text):\n",
    "    text = text.lower()\n",
    "    if 'love' in text:\n",
    "        return 'positive'\n",
    "    elif 'excellent' in text:\n",
    "        return 'positive'\n",
    "    elif 'okay' in text:\n",
    "        return 'neutral'\n",
    "    elif 'Amazing' in text:\n",
    "        return 'positive'\n",
    "    elif 'terrible' in text:\n",
    "        return 'negative'\n",
    "    elif 'Disappointing' in text:\n",
    "        return 'negative'\n",
    "    elif 'Neutral' in text:\n",
    "        return 'neutral'\n",
    "    elif 'dislike' in text:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "result_dict = ({'text':data,'label':[sen(text) for text in data]})\n",
    "\n",
    "for text,data in zip(result_dict['text'],result_dict['label']):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Label: {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b729c7",
   "metadata": {},
   "source": [
    "# Program 8:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a1b921",
   "metadata": {},
   "source": [
    "Implement Python programs for extractive and abstractive summarization \n",
    "without using predefined NLP libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c294cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON - The Trump administration has ordered the military to start withdrawing roughly 7,000 troops from Afghanistan in\n",
      "the coming months, two defense officials said Thursday, an abrupt shift in the 17-year-old war there and a decision that stunned Afghan officials, who\n",
      "said they had not been briefed on the plans.Though Pentagon officials have said the influx of forces - coupled with a more aggressive air campaign - was helping the war effort, Afghan forces\n",
      "continued to take nearly unsustainable levels of casualties and lose ground to the Taliban.The announcement came hours after Jim Mattis, the secretary of defense, said that he would resign from his position at the end of February after\n",
      "disagreeing with the president over his approach to policy in the Middle East.\n"
     ]
    }
   ],
   "source": [
    "def summarizerr(article,num_sentences=3):\n",
    "    sentences = article.split('.')\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    scores = [len(sentence) for sentence in sentences]\n",
    "    selected = sorted(zip(sentences,scores),key=lambda x:x[1],reverse=True)[:num_sentences]\n",
    "    summary = [sentence for sentence,_ in selected]\n",
    "    return '.'.join(summary)+'.'\n",
    "\n",
    "article = \"\"\"\n",
    "WASHINGTON - The Trump administration has ordered the military to start withdrawing roughly 7,000 troops from Afghanistan in\n",
    "the coming months, two defense officials said Thursday, an abrupt shift in the 17-year-old war there and a decision that stunned Afghan officials, who\n",
    "said they had not been briefed on the plans.\n",
    "President Trump made the decision to pull the troops - about half the number the United States has in Afghanistan now - at the same time he\n",
    "decided to pull American forces out of Syria, one official said.\n",
    "The announcement came hours after Jim Mattis, the secretary of defense, said that he would resign from his position at the end of February after\n",
    "disagreeing with the president over his approach to policy in the Middle East.\n",
    "The whirlwind of troop withdrawals and the resignation of Mr. Mattis leave a murky picture for what is next in the United States’ longest war, and they\n",
    "come as Afghanistan has been troubled by spasms of violence afflicting the capital, Kabul, and other important areas.\n",
    "The United States has also been conducting talks with representatives of the Taliban, in what officials have described as discussions that could\n",
    "lead to formal talks to end the conflict.\n",
    "Senior Afghan officials and Western diplomats in Kabul woke up to the shock of the news on Friday morning, and many of them braced for chaos ahead.\n",
    "Several Afghan officials, often in the loop on security planning and decision-making, said they had received no indication in recent days that\n",
    "the Americans would pull troops out.\n",
    "The fear that Mr. Trump might take impulsive actions, however, often loomed in the background of discussions with the United States, they said.\n",
    "They saw the abrupt decision as a further sign that voices from the ground were lacking in the debate over the war and that with Mr. Mattis’s\n",
    "resignation, Afghanistan had lost one of the last influential voices in Washington who channeled the reality of the conflict into the White House’s deliberations.\n",
    "The president long campaigned on bringing troops home, but in 2017, at the request of Mr. Mattis, he begrudgingly pledged an additional 4,000\n",
    "troops to the Afghan campaign to try to hasten an end to the conflict.\n",
    "Though Pentagon officials have said the influx of forces - coupled with a more aggressive air campaign - was helping the war effort, Afghan forces\n",
    "continued to take nearly unsustainable levels of casualties and lose ground to the Taliban.\n",
    "The renewed American effort in 2017 was the first step in ensuring Afghan forces could become more independent without a set timeline for\n",
    "a withdrawal.\n",
    "But with plans to quickly reduce the number of American troops in the country, it is unclear if the Afghans can hold their own against an\n",
    "increasingly aggressive Taliban.\n",
    "Currently, American airstrikes are at levels not seen since the height of the war, when tens of thousands of American troops were spread\n",
    "throughout the country.\n",
    "That air support, officials say, consists mostly of propping up Afghan troops while they try to hold territory from a resurgent Taliban.\n",
    "\"\"\"\n",
    "\n",
    "# Perform summarization\n",
    "summary =  summarizerr(article)\n",
    "\n",
    "# Print the summary\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aa3a5587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def summarize(text, num_sentences=3):\n",
    "    # Preprocess and tokenize\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    word_freq = defaultdict(int)\n",
    "    for word in words:\n",
    "        word_freq[word] += 1\n",
    "\n",
    "    # Score sentences\n",
    "    sentence_scores = []\n",
    "    for sentence in sentences:\n",
    "        score = sum(word_freq[word.lower()] for word in re.findall(r'\\w+', sentence))\n",
    "        sentence_scores.append((sentence, score))\n",
    "\n",
    "    # Sort sentences by score and select top ones\n",
    "    summary_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)[:num_sentences]\n",
    "\n",
    "    # Reorder selected sentences based on their original position\n",
    "    summary = [sentence for sentence, score in sorted(summary_sentences, key=lambda x: sentences.index(x[0]))]\n",
    "\n",
    "    return ' '.join(summary)\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language. The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\"\"\"\n",
    "\n",
    "summary = summarize(text, 2)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce3492",
   "metadata": {},
   "source": [
    "# Program 9:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc2519b",
   "metadata": {},
   "source": [
    "Perform NER without using any pre defined NLP packages \n",
    "The text corpus given below : \n",
    " \n",
    "\"The capital of [France] is [Paris], a city known for its iconic [Eiffel Tower] \n",
    "[John Smith] visited [Tokyo] last summer, exploring the bustling streets of \n",
    "[Shibuya Crossing]. [May 5th, 2023] marks the anniversary of a significant \n",
    "event in [history]. [Elon Musk] is the CEO of [SpaceX] and [Tesla].\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "391d354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persons: ['John Smith', 'Elon Musk']\n",
      "Places: ['France', 'Paris', 'Eiffel Tower', 'Tokyo', 'Shibuya Crossing']\n",
      "Organizations: ['SpaceX', 'Tesla']\n",
      "Dates: ['May 5th, 2023']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The capital of France is Paris, a city known for its iconic Eiffel Tower John Smith visited Tokyo last summer, exploring the bustling streets of Shibuya Crossing. [May 5th, 2023] marks the anniversary of a significant event in history. Elon Musk is the CEO of SpaceX and Tesla.\"\n",
    "\n",
    "# Initialize lists\n",
    "person = []\n",
    "place = []\n",
    "organization = []\n",
    "date = []\n",
    "\n",
    "# Define lists of known entities\n",
    "known_persons = ['John Smith', 'Elon Musk']\n",
    "known_places = ['France', 'Paris', 'Eiffel Tower', 'Tokyo', 'Shibuya Crossing']\n",
    "known_organizations = ['SpaceX', 'Tesla']\n",
    "known_dates = ['May 5th, 2023']\n",
    "\n",
    "# Function to find entities in the sentence\n",
    "def find_entities(sentence, known_entities):\n",
    "    found_entities = []\n",
    "    for entity in known_entities:\n",
    "        if re.search(r'\\b' + re.escape(entity) + r'\\b', sentence):\n",
    "            found_entities.append(entity)\n",
    "    return found_entities\n",
    "\n",
    "# Find entities\n",
    "person = find_entities(sentence, known_persons)\n",
    "place = find_entities(sentence, known_places)\n",
    "organization = find_entities(sentence, known_organizations)\n",
    "date = find_entities(sentence, known_dates)\n",
    "\n",
    "print(\"Persons:\", person)\n",
    "print(\"Places:\", place)\n",
    "print(\"Organizations:\", organization)\n",
    "print(\"Dates:\", date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bf5b38a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON: ['Eiffel Tower John Smith', 'Shibuya Crossing', 'Elon Musk']\n",
      "PLACE: ['France', 'Paris', 'Eiffel Tower', 'Tokyo', 'Shibuya Crossing']\n",
      "ORGANIZATION: ['SpaceX', 'Tesla']\n",
      "DATE: ['May 5th, 2023']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentence = \"The capital of France is Paris, a city known for its iconic Eiffel Tower John Smith visited Tokyo last summer, exploring the bustling streets of Shibuya Crossing. [May 5th, 2023] marks the anniversary of a significant event in history. Elon Musk is the CEO of SpaceX and Tesla.\"\n",
    "\n",
    "# Initialize dictionaries to store entities\n",
    "entities = {\n",
    "    'PERSON': [],\n",
    "    'PLACE': [],\n",
    "    'ORGANIZATION': [],\n",
    "    'DATE': []\n",
    "}\n",
    "\n",
    "# Define patterns for different entity types\n",
    "patterns = {\n",
    "    'PERSON': r'\\b(?:[A-Z][a-z]+ )+(?:[A-Z][a-z]+)\\b',\n",
    "    'PLACE': r'\\b(?:France|Paris|Tokyo|Shibuya Crossing|Eiffel Tower)\\b',\n",
    "    'ORGANIZATION': r'\\b(?:SpaceX|Tesla)\\b',\n",
    "    'DATE': r'\\[([A-Z][a-z]+ \\d{1,2}(?:st|nd|rd|th)?, \\d{4})\\]'\n",
    "}\n",
    "\n",
    "# Find and categorize entities\n",
    "for entity_type, pattern in patterns.items():\n",
    "    matches = re.findall(pattern, sentence)\n",
    "    entities[entity_type].extend(matches)\n",
    "\n",
    "# Remove brackets from date\n",
    "entities['DATE'] = [date.strip('[]') for date in entities['DATE']]\n",
    "\n",
    "# Print results\n",
    "for entity_type, entity_list in entities.items():\n",
    "    print(f\"{entity_type}: {entity_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec222a6c",
   "metadata": {},
   "source": [
    "# Program 10:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd1175",
   "metadata": {},
   "source": [
    "Perform Morphological analysis without using any pre defined NLP \n",
    "packages \n",
    "The text corpus given below : \n",
    " \n",
    "\"The quick brown foxes jumped over the lazy dogs. Mary's cat is playing with \n",
    "a ball. Running swiftly, the athlete won the race. The painted houses lined the \n",
    "street, attracting curious onlookers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "39e13ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['The', 'quick', 'brown', 'foxes', 'jumped', 'over', 'the', 'lazy', 'dogs.', \"Mary's\", 'cat', 'is', 'playing', 'with', 'a', 'ball.', 'Running', 'swiftly,', 'the', 'athlete', 'won', 'the', 'race.', 'The', 'painted', 'houses', 'lined', 'the', 'street,', 'attracting', 'curious', 'onlookers.']\n",
      "Stemmed words: ['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dogs.', \"Mary'\", 'cat', 'i', 'play', 'with', 'a', 'ball.', 'Runn', 'swiftly,', 'the', 'athlete', 'won', 'the', 'race.', 'The', 'painted', 'hous', 'lined', 'the', 'street,', 'attract', 'curiou', 'onlookers.']\n",
      "Lemmatized words: ['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dogs.', \"Mary'\", 'cat', 'i', 'play', 'with', 'a', 'ball.', 'Runn', 'swiftly,', 'the', 'athlete', 'won', 'the', 'race.', 'The', 'painted', 'hous', 'lined', 'the', 'street,', 'attract', 'curiou', 'onlookers.']\n",
      "\n",
      "\n",
      "Word: misunderstanding\n",
      "Morphemes: ['mis', 'understand', 'ing']\n"
     ]
    }
   ],
   "source": [
    "def simple_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def simple_porter_stemmer(word):\n",
    "    # A simple stemming function (for illustration purposes)\n",
    "    if word.endswith(\"es\"):\n",
    "        return word[:-2]\n",
    "    elif word.endswith(\"s\"):\n",
    "        return word[:-1]\n",
    "    elif word.endswith(\"ing\"):\n",
    "        return word[:-3]\n",
    "    return word\n",
    "\n",
    "def simple_wordnet_lemmatizer(word):\n",
    "    # A simple lemmatization function (for illustration purposes)\n",
    "    if word.endswith(\"es\"):\n",
    "        return word[:-2]\n",
    "    elif word.endswith(\"s\"):\n",
    "        return word[:-1]\n",
    "    elif word.endswith(\"ing\"):\n",
    "        return word[:-3]\n",
    "    return word\n",
    "\n",
    "def analyze_morphemes(word, prefixes, root, suffixes):\n",
    "    morphemes = []\n",
    "    for prefix in prefixes:\n",
    "        if word.startswith(prefix):\n",
    "            morphemes.append(prefix)\n",
    "            word = word[len(prefix):]\n",
    "    morphemes.append(root)\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            morphemes.append(suffix)\n",
    "            word = word[:-len(suffix)]\n",
    "    return morphemes\n",
    "\n",
    "#add the data given\n",
    "text = \"The quick brown foxes jumped over the lazy dogs. Mary's cat is playing with a ball. Running swiftly, the athlete won the race. The painted houses lined the street, attracting curious onlookers.\"\n",
    "words = simple_tokenizer(text)\n",
    "stemmed_words = [simple_porter_stemmer(word) for word in words]\n",
    "lemmatized_words = [simple_wordnet_lemmatizer(word) for word in words]\n",
    "\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)\n",
    "print(\"Lemmatized words:\", lemmatized_words)\n",
    "print(\"\\n\")\n",
    "\n",
    "word = \"misunderstanding\"\n",
    "prefixes = [\"mis\"]\n",
    "root = \"understand\"\n",
    "suffixes = [\"ing\"]\n",
    "morphemes = analyze_morphemes(word, prefixes, root, suffixes)\n",
    "\n",
    "print(\"Word:\", word)\n",
    "print(\"Morphemes:\", morphemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7c281e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: The\n",
      "  Length: 3\n",
      "  Capitalized: True\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: quick\n",
      "  Length: 5\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: brown\n",
      "  Length: 5\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: foxes\n",
      "  Length: 5\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: s\n",
      "  Prefix: \n",
      "\n",
      "Word: jumped\n",
      "  Length: 6\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: ed\n",
      "  Prefix: \n",
      "\n",
      "Word: over\n",
      "  Length: 4\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: over\n",
      "\n",
      "Word: the\n",
      "  Length: 3\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: lazy\n",
      "  Length: 4\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: dogs\n",
      "  Length: 4\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: s\n",
      "  Prefix: \n",
      "\n",
      "Word: Mary's\n",
      "  Length: 6\n",
      "  Capitalized: True\n",
      "  Has Apostrophe: True\n",
      "  Suffix: s\n",
      "  Prefix: \n",
      "\n",
      "Word: cat\n",
      "  Length: 3\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: is\n",
      "  Length: 2\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: s\n",
      "  Prefix: \n",
      "\n",
      "Word: playing\n",
      "  Length: 7\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: ing\n",
      "  Prefix: \n",
      "\n",
      "Word: with\n",
      "  Length: 4\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: a\n",
      "  Length: 1\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: ball\n",
      "  Length: 4\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: Running\n",
      "  Length: 7\n",
      "  Capitalized: True\n",
      "  Has Apostrophe: False\n",
      "  Suffix: ing\n",
      "  Prefix: \n",
      "\n",
      "Word: swiftly\n",
      "  Length: 7\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: ly\n",
      "  Prefix: \n",
      "\n",
      "Word: the\n",
      "  Length: 3\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: athlete\n",
      "  Length: 7\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: won\n",
      "  Length: 3\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: the\n",
      "  Length: 3\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: race\n",
      "  Length: 4\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: The\n",
      "  Length: 3\n",
      "  Capitalized: True\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: painted\n",
      "  Length: 7\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: ed\n",
      "  Prefix: \n",
      "\n",
      "Word: houses\n",
      "  Length: 6\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: s\n",
      "  Prefix: \n",
      "\n",
      "Word: lined\n",
      "  Length: 5\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: ed\n",
      "  Prefix: \n",
      "\n",
      "Word: the\n",
      "  Length: 3\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: street\n",
      "  Length: 6\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: \n",
      "  Prefix: \n",
      "\n",
      "Word: attracting\n",
      "  Length: 10\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: ing\n",
      "  Prefix: \n",
      "\n",
      "Word: curious\n",
      "  Length: 7\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: s\n",
      "  Prefix: \n",
      "\n",
      "Word: onlookers\n",
      "  Length: 9\n",
      "  Capitalized: False\n",
      "  Has Apostrophe: False\n",
      "  Suffix: s\n",
      "  Prefix: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def morphological_analysis(text):\n",
    "    words = re.findall(r'\\b[\\w\\']+\\b', text)\n",
    "    analysis = []\n",
    "\n",
    "    for word in words:\n",
    "        features = {\n",
    "            'word': word,\n",
    "            'length': len(word),\n",
    "            'is_capitalized': word[0].isupper(),\n",
    "            'has_apostrophe': \"'\" in word,\n",
    "            'suffix': '',\n",
    "            'prefix': ''\n",
    "        }\n",
    "\n",
    "        # Check for common suffixes\n",
    "        suffixes = ['ing', 'ed', 's', 'es', 'ly', 'ion']\n",
    "        for suffix in suffixes:\n",
    "            if word.endswith(suffix):\n",
    "                features['suffix'] = suffix\n",
    "                break\n",
    "\n",
    "        # Check for common prefixes\n",
    "        prefixes = ['un', 're', 'dis', 'over']\n",
    "        for prefix in prefixes:\n",
    "            if word.startswith(prefix):\n",
    "                features['prefix'] = prefix\n",
    "                break\n",
    "\n",
    "        analysis.append(features)\n",
    "\n",
    "    return analysis\n",
    "\n",
    "text = \"The quick brown foxes jumped over the lazy dogs. Mary's cat is playing with a ball. Running swiftly, the athlete won the race. The painted houses lined the street, attracting curious onlookers.\"\n",
    "\n",
    "result = morphological_analysis(text)\n",
    "\n",
    "for item in result:\n",
    "    print(f\"Word: {item['word']}\")\n",
    "    print(f\"  Length: {item['length']}\")\n",
    "    print(f\"  Capitalized: {item['is_capitalized']}\")\n",
    "    print(f\"  Has Apostrophe: {item['has_apostrophe']}\")\n",
    "    print(f\"  Suffix: {item['suffix']}\")\n",
    "    print(f\"  Prefix: {item['prefix']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a2fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
